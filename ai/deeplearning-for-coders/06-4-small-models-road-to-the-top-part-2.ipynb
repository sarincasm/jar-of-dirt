{"cells":[{"cell_type":"markdown","metadata":{},"source":["Learning notebook based on [this notebook](https://www.kaggle.com/code/jhoward/small-models-road-to-the-top-part-2/) by Jeremy Howard.\n","See a hosted version [on kaggle here](https://www.kaggle.com/code/sarincasm/small-models-road-to-the-top-part-2/edit)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# install fastkaggle if not available\n","try: import fastkaggle\n","except ModuleNotFoundError:\n","    !pip install -q fastkaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:34:18.743030Z","iopub.status.busy":"2024-08-30T07:34:18.742677Z","iopub.status.idle":"2024-08-30T07:34:30.126923Z","shell.execute_reply":"2024-08-30T07:34:30.125757Z","shell.execute_reply.started":"2024-08-30T07:34:18.742940Z"},"trusted":true},"outputs":[],"source":["!pip install -Uq fastai \"timm==0.6.13\" huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-08-30T07:35:18.380721Z","iopub.status.busy":"2024-08-30T07:35:18.379679Z","iopub.status.idle":"2024-08-30T07:35:18.405698Z","shell.execute_reply":"2024-08-30T07:35:18.404938Z","shell.execute_reply.started":"2024-08-30T07:35:18.380667Z"},"trusted":true},"outputs":[],"source":["from fastkaggle import *"]},{"cell_type":"markdown","metadata":{},"source":["This is part 2 of the [Road to the Top](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1) series. Check out [part 1](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1)."]},{"cell_type":"markdown","metadata":{},"source":["## Going faster"]},{"cell_type":"markdown","metadata":{},"source":["First we'll repeat the steps we used last time to access the data and ensure all the latest libraries are installed:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-08-30T07:35:23.659101Z","iopub.status.busy":"2024-08-30T07:35:23.658359Z","iopub.status.idle":"2024-08-30T07:35:25.325715Z","shell.execute_reply":"2024-08-30T07:35:25.324529Z","shell.execute_reply.started":"2024-08-30T07:35:23.659066Z"},"trusted":true},"outputs":[],"source":["comp = 'paddy-disease-classification'\n","path = setup_comp(comp)\n","from fastai.vision.all import *\n","set_seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["A big issue I noticed last time was that originally I created the notebook on my home PC, and each epoch of the resnet we created took under 20 seconds to run. But on Kaggle they took over 3 minutes each! Whilst Kaggle's GPUs are less powerful than what I've got at home, that doesn't come close to explaining this vast difference in speed.\n","\n","I noticed when Kaggle was running that the \"GPU\" indicator in the top right was nearly empty, and the \"CPU\" one was always full. This strongly suggests that the problem was that Kaggle's notebook was CPU bound by decoding and resizing the images. This is a common problem on machines with poor CPU performance -- and indeed Kaggle only provides 2 virtual CPUs at the time of writing.\n","\n","We really need to fix this, since we need to be able to iterate much more quickly. What we can do is to simply resize all the images to half their height and width -- which reduces their number of pixels 4x. This should mean an around 4x increase in performance for training small models.\n","\n","Luckily, fastai has a function which does exactly this, whilst maintaining the folder structure of the data: `resize_images`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:35:28.193626Z","iopub.status.busy":"2024-08-30T07:35:28.192746Z","iopub.status.idle":"2024-08-30T07:35:28.199325Z","shell.execute_reply":"2024-08-30T07:35:28.198249Z","shell.execute_reply.started":"2024-08-30T07:35:28.193583Z"},"trusted":true},"outputs":[],"source":["trn_path = Path('sml')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:35:29.395895Z","iopub.status.busy":"2024-08-30T07:35:29.395058Z","iopub.status.idle":"2024-08-30T07:36:46.789311Z","shell.execute_reply":"2024-08-30T07:36:46.788352Z","shell.execute_reply.started":"2024-08-30T07:35:29.395858Z"},"trusted":true},"outputs":[],"source":["resize_images(path/'train_images', dest=trn_path, max_size=256, recurse=True)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-06-19T01:51:16.072263Z","iopub.status.busy":"2022-06-19T01:51:16.070053Z","iopub.status.idle":"2022-06-19T01:52:00.813437Z","shell.execute_reply":"2022-06-19T01:52:00.81252Z","shell.execute_reply.started":"2022-06-19T01:51:16.072224Z"}},"source":["This will give us 192x256px images. Let's take a look:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:37:30.930381Z","iopub.status.busy":"2024-08-30T07:37:30.929965Z","iopub.status.idle":"2024-08-30T07:37:34.030019Z","shell.execute_reply":"2024-08-30T07:37:34.029222Z","shell.execute_reply.started":"2024-08-30T07:37:30.930340Z"},"trusted":true},"outputs":[],"source":["dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n","    item_tfms=Resize((256,192)))\n","\n","dls.show_batch(max_n=3)"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook, we'll be experimenting with a few different architectures and image processing approaches (item and batch transforms). In order to make this easier, we'll put our modeling steps together into a little function which we can pass the architecture, item transforms, and batch transforms to:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:37:39.138222Z","iopub.status.busy":"2024-08-30T07:37:39.137318Z","iopub.status.idle":"2024-08-30T07:37:39.144039Z","shell.execute_reply":"2024-08-30T07:37:39.143029Z","shell.execute_reply.started":"2024-08-30T07:37:39.138179Z"},"trusted":true},"outputs":[],"source":["def train(arch, item, batch, epochs=5):\n","    dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch)\n","    learn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\n","    learn.fine_tune(epochs, 0.01)\n","    return learn"]},{"cell_type":"markdown","metadata":{},"source":["Our `item_tfms` already resize our images to small sizes, so this shouldn't impact the accuracy of our models much, if at all. Let's re-run our resnet26d to test."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:37:44.237189Z","iopub.status.busy":"2024-08-30T07:37:44.236788Z","iopub.status.idle":"2024-08-30T07:40:22.388619Z","shell.execute_reply":"2024-08-30T07:40:22.387526Z","shell.execute_reply.started":"2024-08-30T07:37:44.237152Z"},"trusted":true},"outputs":[],"source":["learn = train('resnet26d', item=Resize(192),\n","              batch=aug_transforms(size=128, min_scale=0.75))"]},{"cell_type":"markdown","metadata":{},"source":["That's a big improvement in speed, and the accuracy looks fine."]},{"cell_type":"markdown","metadata":{},"source":["## A ConvNeXt model"]},{"cell_type":"markdown","metadata":{},"source":["I noticed that the GPU usage bar in Kaggle was still nearly empty, so we're still CPU bound. That means we should be able to use a more capable model with little if any speed impact. Let's look again at the options in [The best vision models for fine-tuning](https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning). `convnext_small` tops the performance/accuracy tradeoff score there, so let's give it a go!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:47:31.188799Z","iopub.status.busy":"2024-08-30T07:47:31.188370Z","iopub.status.idle":"2024-08-30T07:47:31.193392Z","shell.execute_reply":"2024-08-30T07:47:31.192490Z","shell.execute_reply.started":"2024-08-30T07:47:31.188760Z"},"trusted":true},"outputs":[],"source":["arch = 'convnext_small_in22k'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T07:47:48.266276Z","iopub.status.busy":"2024-08-30T07:47:48.265876Z","iopub.status.idle":"2024-08-30T07:52:27.602892Z","shell.execute_reply":"2024-08-30T07:52:27.601852Z","shell.execute_reply.started":"2024-08-30T07:47:48.266232Z"},"trusted":true},"outputs":[],"source":["learn = train(arch, item=Resize(192, method='squish'),\n","              batch=aug_transforms(size=128, min_scale=0.75))"]},{"cell_type":"markdown","metadata":{},"source":["Wow our error rate has halved! That's a great result. And, as expected, the speed hasn't gone up much at all. This seems like a great model for iterating on."]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing experiments"]},{"cell_type":"markdown","metadata":{},"source":["So, what shall we try first? One thing which can make a difference is whether we \"squish\" a rectangular image into a square shape by changing it's aspect ratio, or randomly crop out a square from it, or whether we add black padding to the edges to make it a square. In the previous version we \"squished\". Let's try \"crop\" instead, which is fastai's default:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:02:41.307477Z","iopub.status.busy":"2024-08-30T08:02:41.307027Z","iopub.status.idle":"2024-08-30T08:07:20.417050Z","shell.execute_reply":"2024-08-30T08:07:20.414295Z","shell.execute_reply.started":"2024-08-30T08:02:41.307435Z"},"trusted":true},"outputs":[],"source":["learn = train(arch, item=Resize(192),\n","              batch=aug_transforms(size=128, min_scale=0.75))"]},{"cell_type":"markdown","metadata":{},"source":["That doesn't seem to have made much difference...\n","\n","We can also try padding, which keeps all the original image without transforming it -- here's what that looks like:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:09:07.404767Z","iopub.status.busy":"2024-08-30T08:09:07.404241Z","iopub.status.idle":"2024-08-30T08:09:08.900457Z","shell.execute_reply":"2024-08-30T08:09:08.899566Z","shell.execute_reply.started":"2024-08-30T08:09:07.404717Z"},"trusted":true},"outputs":[],"source":["dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n","    item_tfms=Resize(192, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros))\n","dls.show_batch(max_n=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:09:27.477217Z","iopub.status.busy":"2024-08-30T08:09:27.476498Z","iopub.status.idle":"2024-08-30T08:15:01.579431Z","shell.execute_reply":"2024-08-30T08:15:01.577542Z","shell.execute_reply.started":"2024-08-30T08:09:27.477181Z"},"trusted":true},"outputs":[],"source":["learn = train(arch, item=Resize((256,192), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n","      batch=aug_transforms(size=(171,128), min_scale=0.75))"]},{"cell_type":"markdown","metadata":{},"source":["That's looking like a pretty good improvement."]},{"cell_type":"markdown","metadata":{},"source":["## Test time augmentation"]},{"cell_type":"markdown","metadata":{},"source":["To make the predictions even better, we can try [test time augmentation](https://nbviewer.org/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb#Test-Time-Augmentation) (TTA), which [our book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) defines as:\n","\n","> *During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.*\n","\n","Before trying that out, we'll first see how to check the predictions and error rate of our model without TTA:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:17:38.750306Z","iopub.status.busy":"2024-08-30T08:17:38.749879Z","iopub.status.idle":"2024-08-30T08:17:43.914878Z","shell.execute_reply":"2024-08-30T08:17:43.914006Z","shell.execute_reply.started":"2024-08-30T08:17:38.750266Z"},"trusted":true},"outputs":[],"source":["valid = learn.dls.valid\n","preds,targs = learn.get_preds(dl=valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:17:54.458191Z","iopub.status.busy":"2024-08-30T08:17:54.457682Z","iopub.status.idle":"2024-08-30T08:17:54.468262Z","shell.execute_reply":"2024-08-30T08:17:54.467338Z","shell.execute_reply.started":"2024-08-30T08:17:54.458144Z"},"trusted":true},"outputs":[],"source":["error_rate(preds, targs)"]},{"cell_type":"markdown","metadata":{},"source":["That's the same error rate we saw at the end of training, above, so we know that we're doing that correctly.\n","\n","Here's what our data augmentation is doing -- if you look carefully, you can see that each image is a bit lighter or darker, sometimes flipped, zoomed, rotated, warped, and/or zoomed:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:18:09.180820Z","iopub.status.busy":"2024-08-30T08:18:09.179929Z","iopub.status.idle":"2024-08-30T08:18:09.802461Z","shell.execute_reply":"2024-08-30T08:18:09.801502Z","shell.execute_reply.started":"2024-08-30T08:18:09.180782Z"},"trusted":true},"outputs":[],"source":["learn.dls.train.show_batch(max_n=6, unique=True)"]},{"cell_type":"markdown","metadata":{},"source":["If we call `tta()` then we'll get the average of predictions made for multiple different augmented versions of each image, along with the unaugmented original:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:18:59.440672Z","iopub.status.busy":"2024-08-30T08:18:59.439719Z","iopub.status.idle":"2024-08-30T08:19:28.943690Z","shell.execute_reply":"2024-08-30T08:19:28.942439Z","shell.execute_reply.started":"2024-08-30T08:18:59.440624Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["tta_preds,_ = learn.tta(dl=valid)"]},{"cell_type":"markdown","metadata":{},"source":["Let's check the error rate of this:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:21:40.794131Z","iopub.status.busy":"2024-08-30T08:21:40.793694Z","iopub.status.idle":"2024-08-30T08:21:40.802841Z","shell.execute_reply":"2024-08-30T08:21:40.801901Z","shell.execute_reply.started":"2024-08-30T08:21:40.794087Z"},"trusted":true},"outputs":[],"source":["error_rate(tta_preds, targs)"]},{"cell_type":"markdown","metadata":{},"source":["That's a huge improvement! We'll definitely want to use this for any submission we make!"]},{"cell_type":"markdown","metadata":{},"source":["## Scaling up"]},{"cell_type":"markdown","metadata":{},"source":["Now that we've got a pretty good model and preprocessing approach, let's scale it up to larger images and more epochs. We'll switch back our path to the original un-resized images, and use 12 epochs using our best settings so far, with larger final augmented images:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:21:54.946801Z","iopub.status.busy":"2024-08-30T08:21:54.945935Z","iopub.status.idle":"2024-08-30T08:21:54.951340Z","shell.execute_reply":"2024-08-30T08:21:54.950365Z","shell.execute_reply.started":"2024-08-30T08:21:54.946753Z"},"trusted":true},"outputs":[],"source":["trn_path = path/'train_images'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:22:07.627786Z","iopub.status.busy":"2024-08-30T08:22:07.626801Z","iopub.status.idle":"2024-08-30T08:49:02.950940Z","shell.execute_reply":"2024-08-30T08:49:02.949549Z","shell.execute_reply.started":"2024-08-30T08:22:07.627736Z"},"trusted":true},"outputs":[],"source":["learn = train(arch, epochs=12,\n","              item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n","              batch=aug_transforms(size=(256,192), min_scale=0.75))"]},{"cell_type":"markdown","metadata":{},"source":["This is around twice as accurate as our previous best model - let's see how it performs with TTA too:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:52:27.315534Z","iopub.status.busy":"2024-08-30T08:52:27.315080Z","iopub.status.idle":"2024-08-30T08:53:54.166740Z","shell.execute_reply":"2024-08-30T08:53:54.165746Z","shell.execute_reply.started":"2024-08-30T08:52:27.315468Z"},"trusted":true},"outputs":[],"source":["tta_preds,targs = learn.tta(dl=learn.dls.valid)\n","error_rate(tta_preds, targs)"]},{"cell_type":"markdown","metadata":{},"source":["Once again, we get a big boost from TTA. This is one of the most under-appreciated deep learning tricks, in my opinion! (I'm not sure there's any other frameworks that make it quite so easy, so perhaps that's part of the reason why...)"]},{"cell_type":"markdown","metadata":{},"source":["## Submission"]},{"cell_type":"markdown","metadata":{},"source":["We're now ready to get our Kaggle submission sorted. First, we'll grab the test set like we did in the last notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:56:27.201450Z","iopub.status.busy":"2024-08-30T08:56:27.201041Z","iopub.status.idle":"2024-08-30T08:56:31.497190Z","shell.execute_reply":"2024-08-30T08:56:31.496513Z","shell.execute_reply.started":"2024-08-30T08:56:27.201411Z"},"trusted":true},"outputs":[],"source":["tst_files = get_image_files(path/'test_images').sorted()\n","tst_dl = learn.dls.test_dl(tst_files)"]},{"cell_type":"markdown","metadata":{},"source":["Next, do TTA on that test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T08:56:35.290413Z","iopub.status.busy":"2024-08-30T08:56:35.289670Z","iopub.status.idle":"2024-08-30T08:58:25.129570Z","shell.execute_reply":"2024-08-30T08:58:25.128689Z","shell.execute_reply.started":"2024-08-30T08:56:35.290375Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["preds,_ = learn.tta(dl=tst_dl)"]},{"cell_type":"markdown","metadata":{},"source":["We need to indices of the largest probability prediction in each row, since that's the index of the predicted disease. `argmax` in PyTorch gives us exactly that:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T09:02:17.665079Z","iopub.status.busy":"2024-08-30T09:02:17.664135Z","iopub.status.idle":"2024-08-30T09:02:17.669801Z","shell.execute_reply":"2024-08-30T09:02:17.668973Z","shell.execute_reply.started":"2024-08-30T09:02:17.665023Z"},"trusted":true},"outputs":[],"source":["idxs = preds.argmax(dim=1)"]},{"cell_type":"markdown","metadata":{},"source":["Now we need to look up those indices in the `vocab`. Last time we did that using pandas, although since then I realised there's an even easier way!:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T09:02:29.652309Z","iopub.status.busy":"2024-08-30T09:02:29.651930Z","iopub.status.idle":"2024-08-30T09:02:29.665453Z","shell.execute_reply":"2024-08-30T09:02:29.664829Z","shell.execute_reply.started":"2024-08-30T09:02:29.652275Z"},"trusted":true},"outputs":[],"source":["vocab = np.array(learn.dls.vocab)\n","results = pd.Series(vocab[idxs], name=\"idxs\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T09:05:31.832030Z","iopub.status.busy":"2024-08-30T09:05:31.830863Z","iopub.status.idle":"2024-08-30T09:05:33.022191Z","shell.execute_reply":"2024-08-30T09:05:33.020899Z","shell.execute_reply.started":"2024-08-30T09:05:31.831977Z"},"trusted":true},"outputs":[],"source":["ss = pd.read_csv(path/'sample_submission.csv')\n","ss['label'] = results\n","ss.to_csv('submission.csv', index=False)\n","!head submission.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if not iskaggle:\n","    from kaggle import api\n","    api.competition_submit_cli('subm.csv', 'convnext small 256x192 12 epochs tta', comp)"]},{"cell_type":"markdown","metadata":{},"source":["This gets a score of 0.9827, which is well within the top 25% of the competition -- that's a big improvement, and we're still using a single small model!"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# This is what I use to push my notebook from my home PC to Kaggle\n","\n","if not iskaggle:\n","    push_notebook('jhoward', 'small-models-road-to-the-top-part-2',\n","                  title='Small models: Road to the Top, Part 2',\n","                  file='small-models-road-to-the-top-part-2.ipynb',\n","                  competition=comp, private=True, gpu=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{},"source":["We've made a big step today, despite just using a single model that trains in under 20 minutes even on Kaggle's rather under-powered machines. Next time, we'll try scaling up to some bigger models and doing some ensembling.\n","\n","If you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. And if you have any questions or comments, please pop them below -- I read every comment I receive!"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":3359805,"sourceId":35325,"sourceType":"competition"}],"dockerImageVersionId":30198,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
